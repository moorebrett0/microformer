# Hyperparameters and config settings

EMBED_DIM = 128        # Size of token embeddings
NUM_HEADS = 4          # Number of attention heads
NUM_LAYERS = 2         # Number of transformer blocks
FF_DIM = 256           # Feedforward layer dimension
MAX_SEQ_LEN = 256      # Maximum sequence length
VOCAB_SIZE = 100       # Placeholder (will be overridden based on dataset)
